{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4803497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from einops import einsum\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eabdc988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        d_k = K.shape[-1]\n",
    "        attention_scores = einsum(\n",
    "            Q, K, \"... query d_k, ... key d_k -> ... query key\"\n",
    "        ) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores = torch.where(mask, attention_scores, float(\"-inf\"))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        return einsum(\n",
    "            attention_weights, V, \"... query key, ... key d_v ->  ... query d_v\"\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_random_inputs(batch_size, seq_len, d_model, device):\n",
    "    Q = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "    K = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "    V = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "    mask = torch.ones(batch_size, seq_len, seq_len, device=device).bool()\n",
    "    return Q, K, V, mask\n",
    "\n",
    "\n",
    "B = 8\n",
    "d_models = [16, 32, 64, 128]\n",
    "seq_lens = [256, 1024, 4096, 8192, 16384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c72bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengze/work/assignment2-systems/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W1208 17:24:08.268000 686475 torch/fx/experimental/symbolic_shapes.py:6833] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s92, 1) | Eq(s92, s24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model=16, seq_len=256: Fwd=0.16ms, Bwd=0.37ms, Mem=467.25MB\n",
      "d_model=16, seq_len=1024: Fwd=0.22ms, Bwd=0.40ms, Mem=442.38MB\n",
      "d_model=16, seq_len=4096: Fwd=3.25ms, Bwd=4.67ms, Mem=1048.75MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 17:24:16.033000 686475 torch/fx/experimental/symbolic_shapes.py:6833] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s92, 1) | Eq(s92, s24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model=16, seq_len=8192: Fwd=13.17ms, Bwd=18.36ms, Mem=2978.25MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 17:24:49.935000 686475 torch/fx/experimental/symbolic_shapes.py:6833] [0/3] _maybe_guard_rel() was called on non-relation expression Eq(s46, 1) | Eq(s46, s60)\n",
      "W1208 17:24:50.038000 686475 torch/fx/experimental/symbolic_shapes.py:6833] [0/3] _maybe_guard_rel() was called on non-relation expression Eq(s92, 1) | Eq(s92, s24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model=16, seq_len=16384: Fwd=52.72ms, Bwd=72.40ms, Mem=10676.25MB\n",
      "d_model=32, seq_len=256: Fwd=0.18ms, Bwd=0.40ms, Mem=411.75MB\n",
      "d_model=32, seq_len=1024: Fwd=0.22ms, Bwd=0.41ms, Mem=444.50MB\n",
      "d_model=32, seq_len=4096: Fwd=3.45ms, Bwd=4.99ms, Mem=1057.25MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1208 17:24:58.492000 686475 torch/fx/experimental/symbolic_shapes.py:6833] [0/4] _maybe_guard_rel() was called on non-relation expression Eq(s46, 1) | Eq(s46, s60)\n",
      "W1208 17:24:58.580000 686475 torch/fx/experimental/symbolic_shapes.py:6833] [0/4] _maybe_guard_rel() was called on non-relation expression Eq(s92, 1) | Eq(s92, s24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model=32, seq_len=8192: Fwd=15.64ms, Bwd=22.25ms, Mem=2996.25MB\n",
      "d_model=32, seq_len=16384: Fwd=54.65ms, Bwd=77.90ms, Mem=10712.25MB\n",
      "d_model=64, seq_len=256: Fwd=0.18ms, Bwd=0.38ms, Mem=420.75MB\n",
      "d_model=64, seq_len=1024: Fwd=0.29ms, Bwd=0.44ms, Mem=448.75MB\n",
      "d_model=64, seq_len=4096: Fwd=2.90ms, Bwd=5.35ms, Mem=1074.25MB\n",
      "d_model=64, seq_len=8192: Fwd=13.66ms, Bwd=21.97ms, Mem=3032.25MB\n",
      "d_model=64, seq_len=16384: Fwd=60.03ms, Bwd=89.50ms, Mem=10784.25MB\n",
      "d_model=128, seq_len=256: Fwd=0.18ms, Bwd=0.38ms, Mem=438.75MB\n",
      "d_model=128, seq_len=1024: Fwd=0.38ms, Bwd=0.57ms, Mem=457.25MB\n",
      "d_model=128, seq_len=4096: Fwd=4.60ms, Bwd=8.40ms, Mem=1108.25MB\n",
      "d_model=128, seq_len=8192: Fwd=20.09ms, Bwd=33.62ms, Mem=3104.25MB\n",
      "d_model=128, seq_len=16384: Fwd=87.35ms, Bwd=136.72ms, Mem=10928.25MB\n",
      "    d_model  seq_len  fwd_time_ms  bwd_time_ms        mem_mb\n",
      "0        16      256     0.162686     0.365602    467.250488\n",
      "1        16     1024     0.216485     0.400899    442.375488\n",
      "2        16     4096     3.250614     4.668465   1048.750488\n",
      "3        16     8192    13.170645    18.360616   2978.250488\n",
      "4        16    16384    52.716768    72.403973  10676.250488\n",
      "5        32      256     0.180307     0.398755    411.750488\n",
      "6        32     1024     0.222639     0.410617    444.500488\n",
      "7        32     4096     3.451696     4.988198   1057.250488\n",
      "8        32     8192    15.641556    22.247633   2996.250488\n",
      "9        32    16384    54.651079    77.900914  10712.250488\n",
      "10       64      256     0.183706     0.380036    420.750488\n",
      "11       64     1024     0.287793     0.437363    448.750488\n",
      "12       64     4096     2.902975     5.346785   1074.250488\n",
      "13       64     8192    13.656846    21.970066   3032.250488\n",
      "14       64    16384    60.029570    89.501708  10784.250488\n",
      "15      128      256     0.181163     0.379542    438.750488\n",
      "16      128     1024     0.376452     0.573461    457.250488\n",
      "17      128     4096     4.597416     8.404842   1108.250488\n",
      "18      128     8192    20.090460    33.619734   3104.250488\n",
      "19      128    16384    87.346680   136.716950  10928.250488\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "results = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = torch.compile(SDPA().to(device))\n",
    "\n",
    "for d_model in d_models:\n",
    "    for seq_len in seq_lens:\n",
    "        try:\n",
    "            # (c) Create random inputs\n",
    "            Q, K, V, mask = generate_random_inputs(B, seq_len, d_model, device)\n",
    "            Q.requires_grad = True\n",
    "            K.requires_grad = True\n",
    "            V.requires_grad = True\n",
    "\n",
    "            # (f) Warm up\n",
    "            for _ in range(5):\n",
    "                out = model(Q, K, V, mask)\n",
    "                loss = out.sum()\n",
    "                loss.backward()\n",
    "                Q.grad = None\n",
    "                K.grad = None\n",
    "                V.grad = None\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # (d) Time 100 forward passes\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start_event.record()\n",
    "            for _ in range(100):\n",
    "                out = model(Q, K, V, mask)\n",
    "                torch.cuda.synchronize()\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize()\n",
    "            fwd_time = start_event.elapsed_time(end_event) / 100\n",
    "\n",
    "            # (e) Measure memory before backward pass\n",
    "            # Run one forward pass to populate the graph and activations\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "            out = model(Q, K, V, mask)\n",
    "            # Memory in use (activations + parameters + inputs)\n",
    "            mem_allocated = torch.cuda.memory_allocated() / (1024**2)  # MB\n",
    "\n",
    "            # (e) Time 100 backward passes\n",
    "            grad_output = torch.randn_like(out)\n",
    "            total_bwd_time = 0\n",
    "\n",
    "            for _ in range(100):\n",
    "                # Re-run forward to ensure valid graph for each backward pass\n",
    "                # We don't time this forward pass\n",
    "                out = model(Q, K, V, mask)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                start_event.record()\n",
    "                out.backward(grad_output)\n",
    "                torch.cuda.synchronize()\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "                total_bwd_time += start_event.elapsed_time(end_event)\n",
    "\n",
    "                # Zero gradients\n",
    "                Q.grad = None\n",
    "                K.grad = None\n",
    "                V.grad = None\n",
    "\n",
    "            bwd_time = total_bwd_time / 100\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"d_model\": d_model,\n",
    "                    \"seq_len\": seq_len,\n",
    "                    \"fwd_time_ms\": fwd_time,\n",
    "                    \"bwd_time_ms\": bwd_time,\n",
    "                    \"mem_mb\": mem_allocated,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"d_model={d_model}, seq_len={seq_len}: Fwd={fwd_time:.2f}ms, Bwd={bwd_time:.2f}ms, Mem={mem_allocated:.2f}MB\"\n",
    "            )\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Failed for d_model={d_model}, seq_len={seq_len}: {e}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdfd6459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "d_model",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "seq_len",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fwd_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bwd_time_ms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mem_mb",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d9986bac-a14e-4493-9091-5d4cc7f73510",
       "rows": [
        [
         "0",
         "16",
         "256",
         "0.1626860809326172",
         "0.3656022399663925",
         "467.25048828125"
        ],
        [
         "1",
         "16",
         "1024",
         "0.21648479461669923",
         "0.4008988791704178",
         "442.37548828125"
        ],
        [
         "2",
         "16",
         "4096",
         "3.2506143188476564",
         "4.668464641571045",
         "1048.75048828125"
        ],
        [
         "3",
         "16",
         "8192",
         "13.17064453125",
         "18.360616359710694",
         "2978.25048828125"
        ],
        [
         "4",
         "16",
         "16384",
         "52.716767578125",
         "72.4039729309082",
         "10676.25048828125"
        ],
        [
         "5",
         "32",
         "256",
         "0.18030656814575197",
         "0.398754880130291",
         "411.75048828125"
        ],
        [
         "6",
         "32",
         "1024",
         "0.22263904571533202",
         "0.4106172809004784",
         "444.50048828125"
        ],
        [
         "7",
         "32",
         "4096",
         "3.451696472167969",
         "4.988197765350342",
         "1057.25048828125"
        ],
        [
         "8",
         "32",
         "8192",
         "15.641556396484376",
         "22.247633266448975",
         "2996.25048828125"
        ],
        [
         "9",
         "32",
         "16384",
         "54.6510791015625",
         "77.90091438293457",
         "10712.25048828125"
        ],
        [
         "10",
         "64",
         "256",
         "0.18370624542236327",
         "0.3800361606478691",
         "420.75048828125"
        ],
        [
         "11",
         "64",
         "1024",
         "0.28779296875",
         "0.437362559735775",
         "448.75048828125"
        ],
        [
         "12",
         "64",
         "4096",
         "2.9029751586914063",
         "5.346784968376159",
         "1074.25048828125"
        ],
        [
         "13",
         "64",
         "8192",
         "13.656845703125",
         "21.97006555557251",
         "3032.25048828125"
        ],
        [
         "14",
         "64",
         "16384",
         "60.0295703125",
         "89.5017079925537",
         "10784.25048828125"
        ],
        [
         "15",
         "128",
         "256",
         "0.1811631965637207",
         "0.37954207926988603",
         "438.75048828125"
        ],
        [
         "16",
         "128",
         "1024",
         "0.3764521789550781",
         "0.5734608006477356",
         "457.25048828125"
        ],
        [
         "17",
         "128",
         "4096",
         "4.597415771484375",
         "8.40484185218811",
         "1108.25048828125"
        ],
        [
         "18",
         "128",
         "8192",
         "20.090460205078124",
         "33.61973449707031",
         "3104.25048828125"
        ],
        [
         "19",
         "128",
         "16384",
         "87.3466796875",
         "136.7169497680664",
         "10928.25048828125"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_model</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>fwd_time_ms</th>\n",
       "      <th>bwd_time_ms</th>\n",
       "      <th>mem_mb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>0.162686</td>\n",
       "      <td>0.365602</td>\n",
       "      <td>467.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.216485</td>\n",
       "      <td>0.400899</td>\n",
       "      <td>442.375488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>4096</td>\n",
       "      <td>3.250614</td>\n",
       "      <td>4.668465</td>\n",
       "      <td>1048.750488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>8192</td>\n",
       "      <td>13.170645</td>\n",
       "      <td>18.360616</td>\n",
       "      <td>2978.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>16384</td>\n",
       "      <td>52.716768</td>\n",
       "      <td>72.403973</td>\n",
       "      <td>10676.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>0.180307</td>\n",
       "      <td>0.398755</td>\n",
       "      <td>411.750488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.222639</td>\n",
       "      <td>0.410617</td>\n",
       "      <td>444.500488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>4096</td>\n",
       "      <td>3.451696</td>\n",
       "      <td>4.988198</td>\n",
       "      <td>1057.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>15.641556</td>\n",
       "      <td>22.247633</td>\n",
       "      <td>2996.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>16384</td>\n",
       "      <td>54.651079</td>\n",
       "      <td>77.900914</td>\n",
       "      <td>10712.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>0.183706</td>\n",
       "      <td>0.380036</td>\n",
       "      <td>420.750488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.287793</td>\n",
       "      <td>0.437363</td>\n",
       "      <td>448.750488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.902975</td>\n",
       "      <td>5.346785</td>\n",
       "      <td>1074.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>8192</td>\n",
       "      <td>13.656846</td>\n",
       "      <td>21.970066</td>\n",
       "      <td>3032.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>64</td>\n",
       "      <td>16384</td>\n",
       "      <td>60.029570</td>\n",
       "      <td>89.501708</td>\n",
       "      <td>10784.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.181163</td>\n",
       "      <td>0.379542</td>\n",
       "      <td>438.750488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>128</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.376452</td>\n",
       "      <td>0.573461</td>\n",
       "      <td>457.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>128</td>\n",
       "      <td>4096</td>\n",
       "      <td>4.597416</td>\n",
       "      <td>8.404842</td>\n",
       "      <td>1108.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>128</td>\n",
       "      <td>8192</td>\n",
       "      <td>20.090460</td>\n",
       "      <td>33.619734</td>\n",
       "      <td>3104.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>128</td>\n",
       "      <td>16384</td>\n",
       "      <td>87.346680</td>\n",
       "      <td>136.716950</td>\n",
       "      <td>10928.250488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    d_model  seq_len  fwd_time_ms  bwd_time_ms        mem_mb\n",
       "0        16      256     0.162686     0.365602    467.250488\n",
       "1        16     1024     0.216485     0.400899    442.375488\n",
       "2        16     4096     3.250614     4.668465   1048.750488\n",
       "3        16     8192    13.170645    18.360616   2978.250488\n",
       "4        16    16384    52.716768    72.403973  10676.250488\n",
       "5        32      256     0.180307     0.398755    411.750488\n",
       "6        32     1024     0.222639     0.410617    444.500488\n",
       "7        32     4096     3.451696     4.988198   1057.250488\n",
       "8        32     8192    15.641556    22.247633   2996.250488\n",
       "9        32    16384    54.651079    77.900914  10712.250488\n",
       "10       64      256     0.183706     0.380036    420.750488\n",
       "11       64     1024     0.287793     0.437363    448.750488\n",
       "12       64     4096     2.902975     5.346785   1074.250488\n",
       "13       64     8192    13.656846    21.970066   3032.250488\n",
       "14       64    16384    60.029570    89.501708  10784.250488\n",
       "15      128      256     0.181163     0.379542    438.750488\n",
       "16      128     1024     0.376452     0.573461    457.250488\n",
       "17      128     4096     4.597416     8.404842   1108.250488\n",
       "18      128     8192    20.090460    33.619734   3104.250488\n",
       "19      128    16384    87.346680   136.716950  10928.250488"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff846909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"compiled_pytorch_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16b528",
   "metadata": {},
   "source": [
    "# Weighted Sum Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01cd9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90926747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import W\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr,\n",
    "    weight_ptr,\n",
    "    output_ptr,\n",
    "    x_stride_row,\n",
    "    x_stride_dim,\n",
    "    weight_stride_dim,\n",
    "    output_stride_row,\n",
    "    ROWS,\n",
    "    D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr,\n",
    "    D_TILE_SIZE: tl.constexpr,\n",
    "):\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(\n",
    "            ROWS,\n",
    "            D,\n",
    "        ),\n",
    "        strides=(\n",
    "            x_stride_row,\n",
    "            x_stride_dim,\n",
    "        ),\n",
    "        offsets=(\n",
    "            row_tile_idx * ROWS_TILE_SIZE,\n",
    "            0,\n",
    "        ),\n",
    "        block_shape=(\n",
    "            ROWS_TILE_SIZE,\n",
    "            D_TILE_SIZE,\n",
    "        ),\n",
    "        order=(\n",
    "            1,\n",
    "            0,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    x_ptr,\n",
    "    weight_ptr,  # Input\n",
    "    grad_output_ptr,  # Grad input\n",
    "    grad_x_ptr,\n",
    "    partial_grad_weight_ptr,  # Grad outputs\n",
    "    stride_xr,\n",
    "    stride_xd,\n",
    "    stride_wr,\n",
    "    stride_gr,\n",
    "    stride_gxr,\n",
    "    stride_gxd,\n",
    "    stride_gwb,\n",
    "    stride_gwd,\n",
    "    NUM_ROWS,\n",
    "    D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr,\n",
    "    D_TILE_SIZE: tl.constexpr,\n",
    "):\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    n_row_tiles = tl.num_programs(0)\n",
    "\n",
    "    # Inputs\n",
    "    grad_output_block_ptr = tl.make_block_ptr(\n",
    "        grad_output_ptr,\n",
    "        shape=(NUM_ROWS,),\n",
    "        strides=(stride_gr,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(\n",
    "            NUM_ROWS,\n",
    "            D,\n",
    "        ),\n",
    "        strides=(\n",
    "            stride_xr,\n",
    "            stride_xd,\n",
    "        ),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(stride_wr,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    grad_x_block_ptr = tl.make_block_ptr(\n",
    "        grad_x_ptr,\n",
    "        shape=(\n",
    "            NUM_ROWS,\n",
    "            D,\n",
    "        ),\n",
    "        strides=(\n",
    "            stride_gxr,\n",
    "            stride_gxd,\n",
    "        ),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    partial_grad_weight_block_ptr = tl.make_block_ptr(\n",
    "        partial_grad_weight_ptr,\n",
    "        shape=(\n",
    "            n_row_tiles,\n",
    "            D,\n",
    "        ),\n",
    "        strides=(\n",
    "            stride_gwb,\n",
    "            stride_gwd,\n",
    "        ),\n",
    "        offsets=(row_tile_idx, 0),\n",
    "        block_shape=(1, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        grad_output = tl.load(\n",
    "            grad_output_block_ptr, boundary_check=(0,), padding_option=\"zero\"\n",
    "        )  # (ROWS_TILE_SIZE,)\n",
    "\n",
    "        # Outer product for grad_x\n",
    "        weight = tl.load(\n",
    "            weight_block_ptr, boundary_check=(0,), padding_option=\"zero\"\n",
    "        )  # (D_TILE_SIZE,)\n",
    "        grad_x_row = grad_output[:, None] * weight[None, :]\n",
    "        tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0, 1))\n",
    "\n",
    "        # Reduce as many rows as possible for the grad_weight result\n",
    "        row = tl.load(\n",
    "            x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"\n",
    "        )  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        grad_weight_row = tl.sum(row * grad_output[:, None], axis=0, keepdim=True)\n",
    "        tl.store(partial_grad_weight_block_ptr, grad_weight_row, boundary_check=(1,))\n",
    "\n",
    "        # Move the pointers to the next tile along D\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
    "        partial_grad_weight_block_ptr = partial_grad_weight_block_ptr.advance(\n",
    "            (0, D_TILE_SIZE)\n",
    "        )\n",
    "        grad_x_block_ptr = grad_x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "\n",
    "\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "        input_shape = x.shape\n",
    "        ROWS = 1\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "        ctx.save_for_backward(x, weight)\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
    "        assert x.is_contiguous(), \"Our pointer arithmetic will assume contiguous x\"\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16\n",
    "        ctx.ROWS_TILE_SIZE = 16\n",
    "        ctx.input_shape = input_shape\n",
    "        y = torch.empty(output_dims, device=x.device)\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x,\n",
    "            weight,\n",
    "            y,\n",
    "            x.stride(0),\n",
    "            x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows,\n",
    "            D=D,\n",
    "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE,\n",
    "            D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
    "        )\n",
    "        return y.view(input_shape[:-1])\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        x, weight = ctx.saved_tensors\n",
    "        ROWS_TILE_SIZE, D_TILE_SIZE = (\n",
    "            ctx.ROWS_TILE_SIZE,\n",
    "            ctx.D_TILE_SIZE,\n",
    "        )  # These don't have to be the same\n",
    "        n_rows, D = x.shape\n",
    "\n",
    "        # Our strategy is for each thread block to first write to a partial buffer,\n",
    "        # then we reduce over this buffer to get the final gradient.\n",
    "        partial_grad_weight = torch.empty(\n",
    "            (triton.cdiv(n_rows, ROWS_TILE_SIZE), D), device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        grad_x = torch.empty_like(x)\n",
    "\n",
    "        weighted_sum_backward[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x,\n",
    "            weight,\n",
    "            grad_out,\n",
    "            grad_x,\n",
    "            partial_grad_weight,\n",
    "            x.stride(0),\n",
    "            x.stride(1),\n",
    "            weight.stride(0),\n",
    "            grad_out.stride(0),\n",
    "            grad_x.stride(0),\n",
    "            grad_x.stride(1),\n",
    "            partial_grad_weight.stride(0),\n",
    "            partial_grad_weight.stride(1),\n",
    "            NUM_ROWS=n_rows,\n",
    "            D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE,\n",
    "            D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        grad_weight = partial_grad_weight.sum(axis=0)\n",
    "        return grad_x, grad_weight\n",
    "\n",
    "\n",
    "f_weightedsum = WeightedSumFunc.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b72ff115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward a shape: torch.Size([128])\n",
      "forward b shape: torch.Size([128])\n",
      "forward max abs diff: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(128, 16).to(\"cuda\")\n",
    "w = torch.rand(16).to(\"cuda\")\n",
    "a = f_weightedsum(x, w)\n",
    "b = weighted_sum(x, w)\n",
    "\n",
    "print(\"forward a shape:\", a.shape)\n",
    "print(\"forward b shape:\", b.shape)\n",
    "print(\"forward max abs diff:\", (a - b).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d531692",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 103:26:\n        # Outer product for grad_x\n        weight = tl.load(\n            weight_block_ptr, boundary_check=(0,), padding_option=\"zero\"\n        )  # (D_TILE_SIZE,)\n        grad_x_row = grad_output[:, None] * weight[None, :]\n        tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0, 1))\n\n        # Reduce as many rows as possible for the grad_weight result\n        row = tl.load(\n            x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"\n        )  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n        grad_weight_row = tl.sum(row * grad_output[:, None], axis=0, keepdim=True)\n                          ^\nTypeError(\"sum() got an unexpected keyword argument 'keepdim'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCompilationError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m a1 = f_weightedsum(x1, w1)\n\u001b[32m      4\u001b[39m loss_a = a1.sum()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mloss_a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m grad_x_a = x1.grad.clone()\n\u001b[32m      7\u001b[39m grad_w_a = w1.grad.clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/torch/autograd/function.py:315\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    310\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m     )\n\u001b[32m    314\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 237\u001b[39m, in \u001b[36mWeightedSumFunc.backward\u001b[39m\u001b[34m(ctx, grad_out)\u001b[39m\n\u001b[32m    232\u001b[39m partial_grad_weight = torch.empty(\n\u001b[32m    233\u001b[39m     (triton.cdiv(n_rows, ROWS_TILE_SIZE), D), device=x.device, dtype=x.dtype\n\u001b[32m    234\u001b[39m )\n\u001b[32m    235\u001b[39m grad_x = torch.empty_like(x)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mweighted_sum_backward\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriton\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mROWS_TILE_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial_grad_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_out\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_x\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_x\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial_grad_weight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial_grad_weight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNUM_ROWS\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mD\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mROWS_TILE_SIZE\u001b[49m\u001b[43m=\u001b[49m\u001b[43mROWS_TILE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mD_TILE_SIZE\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD_TILE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m grad_weight = partial_grad_weight.sum(axis=\u001b[32m0\u001b[39m)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grad_x, grad_weight\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:419\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    414\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:733\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     options, signature, constexprs, attrs = \u001b[38;5;28mself\u001b[39m._pack_args(backend, kwargs, bound_args, specialization,\n\u001b[32m    731\u001b[39m                                                             options)\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m     kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    735\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:861\u001b[39m, in \u001b[36mJITFunction._do_compile\u001b[39m\u001b[34m(self, key, signature, device, constexprs, options, attrs, warmup)\u001b[39m\n\u001b[32m    859\u001b[39m     kernel = async_mode.submit(cache_key, async_compile, finalize_compile)\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m     kernel_cache[key] = kernel\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._call_hook(knobs.runtime.jit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n\u001b[32m    864\u001b[39m                     warmup)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:300\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(src, target, options, _env_vars)\u001b[39m\n\u001b[32m    298\u001b[39m module_map = backend.get_module_map()\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     module = \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    302\u001b[39m     filter_traceback(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/assignment2-systems/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:80\u001b[39m, in \u001b[36mASTSource.make_ir\u001b[39m\u001b[34m(self, target, options, codegen_fns, module_map, context)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, target: GPUTarget, options, codegen_fns, module_map, context):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcode_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ast_to_ttir\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mCompilationError\u001b[39m: at 103:26:\n        # Outer product for grad_x\n        weight = tl.load(\n            weight_block_ptr, boundary_check=(0,), padding_option=\"zero\"\n        )  # (D_TILE_SIZE,)\n        grad_x_row = grad_output[:, None] * weight[None, :]\n        tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0, 1))\n\n        # Reduce as many rows as possible for the grad_weight result\n        row = tl.load(\n            x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"\n        )  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n        grad_weight_row = tl.sum(row * grad_output[:, None], axis=0, keepdim=True)\n                          ^\nTypeError(\"sum() got an unexpected keyword argument 'keepdim'\")"
     ]
    }
   ],
   "source": [
    "x1 = x.clone().detach().requires_grad_(True)\n",
    "w1 = w.clone().detach().requires_grad_(True)\n",
    "a1 = f_weightedsum(x1, w1)\n",
    "loss_a = a1.sum()\n",
    "loss_a.backward()\n",
    "grad_x_a = x1.grad.clone()\n",
    "grad_w_a = w1.grad.clone()\n",
    "\n",
    "#  PyTorch baseline\n",
    "x2 = x.clone().detach().requires_grad_(True)\n",
    "w2 = w.clone().detach().requires_grad_(True)\n",
    "b1 = weighted_sum(x2, w2)\n",
    "loss_b = b1.sum()\n",
    "loss_b.backward()\n",
    "grad_x_b = x2.grad.clone()\n",
    "grad_w_b = w2.grad.clone()\n",
    "\n",
    "print(\"grad_x max abs diff:\", (grad_x_a - grad_x_b).abs().max().item())\n",
    "print(\"grad_w max abs diff:\", (grad_w_a - grad_w_b).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb569932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float, Bool, jaxtyped\n",
    "from beartype import beartype\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "class TorchFlashAttention2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        Q: Float[Tensor, \"batch q_len d_model\"],\n",
    "        K: Float[Tensor, \"batch k_len d_model\"],\n",
    "        V: Float[Tensor, \"batch v_len d_model\"],\n",
    "        mask: Float[Tensor, \"batch q_len k_len\"],\n",
    "    ):\n",
    "        B_q = 32\n",
    "        B_kv = 16\n",
    "        bs, q_len, d_model = Q.shape\n",
    "        T_q = q_len // B_q\n",
    "        T_k = K.shape[1] // B_kv\n",
    "        O = torch.zeros((bs, q_len, d_model))\n",
    "        L = torch.zeros((bs, T_q, B_q))\n",
    "\n",
    "        for b in range(bs):\n",
    "            for i in range(T_q):\n",
    "                q_tile = Q[b, i * B_q : (i + 1) * B_q, :]\n",
    "                o_tile = torch.zeros((B_q, d_model))\n",
    "                l = torch.zeros((B_q,))\n",
    "                m = torch.full((B_q,), -torch.inf)\n",
    "                for j in range(T_k):\n",
    "                    k_tile = K[b, j * B_kv : (j + 1) * B_kv, :]\n",
    "                    v_tile = V[b, j * B_kv : (j + 1) * B_kv, :]\n",
    "                    s_tile = q_tile @ k_tile.T / math.sqrt(d_model)\n",
    "                    row_max = torch.max(s_tile, dim=-1).values\n",
    "                    old_m = m\n",
    "                    m = torch.maximum(row_max, m)\n",
    "                    scale_factor = torch.exp(old_m - m)\n",
    "                    p_tile = torch.exp(s_tile - m.unsqueeze(-1))  #  softmax \n",
    "                    l = scale_factor * l + torch.sum(p_tile, dim=-1)  # softmax \n",
    "                    o_tile = scale_factor.unsqueeze(-1) * o_tile + p_tile @ v_tile\n",
    "                L[b, i, :] = m + torch.log(l)\n",
    "                O[b, i * B_q : (i + 1) * B_q, :] = o_tile / l.unsqueeze(-1)\n",
    "        ctx.save_for_backward(Q, K, V, mask, O, L)\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbfa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float, Bool, jaxtyped\n",
    "from beartype import beartype\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# fmt: off\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, # Inputs\n",
    "    O_ptr, L_ptr, # Outputs\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    IS_CAUSAL: tl.constexpr,\n",
    "): \n",
    "# fmt: on\n",
    "    query_tile_index = tl.program_id(0) # parallize over queries because seq_len dim is embarassingly parallelized\n",
    "    batch_index = tl.program_id(1)\n",
    "    \n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES,),\n",
    "        strides=(stride_lq,),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE,),\n",
    "        block_shape=(Q_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    q = tl.load(Q_block_ptr)\n",
    "    o = tl.zeros((Q_TILE_SIZE, D), dtype=tl.float32)\n",
    "    l = tl.zeros((Q_TILE_SIZE,), dtype=tl.float32)\n",
    "    m = tl.full((Q_TILE_SIZE,), -float(\"inf\"), dtype=tl.float32)\n",
    "    \n",
    "    hi = N_KEYS\n",
    "    if IS_CAUSAL:\n",
    "        hi = min((query_tile_index + 1) * Q_TILE_SIZE, N_KEYS)\n",
    "    \n",
    "    offs_m = query_tile_index * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)   \n",
    "    for start_k in range(0, hi, K_TILE_SIZE):\n",
    "        k = tl.load(K_block_ptr)\n",
    "        v = tl.load(V_block_ptr)\n",
    "        s = tl.dot(q, k.T) * scale\n",
    "        offs_n = start_k + tl.arange(0, K_TILE_SIZE)\n",
    "        if IS_CAUSAL and (start_k + K_TILE_SIZE > query_tile_index * Q_TILE_SIZE):\n",
    "                mask = offs_n[None, :] > offs_m[:, None]\n",
    "                s = tl.where(mask, -1.0e6, s)\n",
    "        row_max = tl.max(s, axis=-1)\n",
    "        old_m = m\n",
    "        m = tl.maximum(row_max, m)\n",
    "        scale_factor = tl.exp(old_m - m)\n",
    "        p = tl.exp(s - m[:, None])\n",
    "        l = scale_factor * l + tl.sum(p, axis=-1)\n",
    "        o = o * scale_factor[:, None]\n",
    "        o = tl.dot(p.to(v.dtype), v, acc=o)\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (K_TILE_SIZE, 0))\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (K_TILE_SIZE, 0))\n",
    "    o = o / l[:, None]\n",
    "    l = m + tl.log(l)\n",
    "    tl.store(O_block_ptr, o.to(O_block_ptr.type.element_ty))\n",
    "    tl.store(L_block_ptr, l)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class TritonFlashAttention2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,\n",
    "        Q: Float[Tensor, \"batch q_len d_model\"],\n",
    "        K: Float[Tensor, \"batch k_len d_model\"],\n",
    "        V: Float[Tensor, \"batch v_len d_model\"],\n",
    "        is_causal: bool\n",
    "    ):\n",
    "        Q_TILE_SIZE = 32\n",
    "        K_TILE_SIZE = 16\n",
    "        bs, N_QUERIES, d_model = Q.shape\n",
    "        N_KEYS = K.shape[1]\n",
    "        T_q = triton.cdiv(N_QUERIES, Q_TILE_SIZE)\n",
    "        O = torch.zeros((bs, N_QUERIES, d_model), device = Q.device)\n",
    "        L = torch.zeros((bs, N_QUERIES), device = Q.device)\n",
    "        grid = (T_q, bs)\n",
    "        flash_fwd_kernel[grid](\n",
    "            Q, K, V, \n",
    "            O, L,Q.stride(0), Q.stride(1), Q.stride(2),\n",
    "            K.stride(0), K.stride(1), K.stride(2),\n",
    "            V.stride(0), V.stride(1), V.stride(2),\n",
    "            O.stride(0), O.stride(1), O.stride(2),\n",
    "            L.stride(0), L.stride(1),\n",
    "            N_QUERIES, N_KEYS,\n",
    "            scale=1.0 / (d_model ** 0.5),\n",
    "            D=d_model, # type: ignore\n",
    "            Q_TILE_SIZE=Q_TILE_SIZE, # type: ignore\n",
    "            K_TILE_SIZE=K_TILE_SIZE, # type: ignore\n",
    "            IS_CAUSAL=is_causal, # type: ignore\n",
    "        )\n",
    "        ctx.save_for_backward(Q, K, V, O, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "from cs336_systems.flash_attn import TorchFlashAttention2\n",
    "\n",
    "f_torch_flash_attn = TorchFlashAttention2.apply\n",
    "f_triton_flash_attn = TritonFlashAttention2.apply\n",
    "# Test\n",
    "bs = 2\n",
    "seq_len = 32\n",
    "d_model = 128\n",
    "size = (bs, seq_len, d_model)\n",
    "\n",
    "\n",
    "with torch.device(\"cuda:0\"):\n",
    "    q = torch.rand(size)\n",
    "    k = torch.rand(size)\n",
    "    v = torch.rand(size)\n",
    "    mask = torch.rand((seq_len, seq_len))\n",
    "    res = f_triton_flash_attn(q, k, v, True)\n",
    "    ref = f_torch_flash_attn(q, k, v, True)\n",
    "    # ref = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "    torch.testing.assert_close(res, ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b390fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
